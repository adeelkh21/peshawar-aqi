name: AQI Model Training Pipeline

on:
  schedule:
    # Run every 6 hours (at 00:00, 06:00, 12:00, 18:00 UTC)
    - cron: '0 */6 * * *'
  workflow_dispatch: # Allow manual trigger
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/aqi_model_training.yml'

permissions:
  contents: write
  pull-requests: write

jobs:
  train-model:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        lfs: true
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create model directories
      run: |
        mkdir -p data_repositories/models
        mkdir -p data_repositories/models/trained_models
        mkdir -p data_repositories/models/evaluation_reports
        mkdir -p deployment/
        
    - name: Verify historical data exists (no synthesis)
      run: |
        python -c "
        import os, sys, logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        historical_file = 'data_repositories/historical_data/real_historical_dataset.csv'
        logger.info('ðŸ“¦ Verifying historical data exists...')
        if not os.path.exists(historical_file):
            logger.error(f'âŒ Historical data file not found: {historical_file}. This workflow never synthesizes data.')
            sys.exit(1)
        logger.info(f'âœ… Historical data found: {historical_file}')
        "
        
    - name: Merge historical and real-time data
      run: |
        python -c "
        import sys
        sys.path.append('.')
        import pandas as pd
        import os
        from datetime import datetime, timedelta
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        logger.info('ðŸ”„ Merging historical and real-time data...')
        
        try:
            # Load historical data (now guaranteed to exist)
            historical_file = 'data_repositories/historical_data/real_historical_dataset.csv'
            realtime_file = 'data_repositories/processed/merged_data.csv'
            
            # Load historical data (150+ days)
            historical_df = pd.read_csv(historical_file)
            logger.info(f'ðŸ“¦ Loaded historical data: {len(historical_df)} records')
            
            # Load recent real-time data
            if os.path.exists(realtime_file):
                realtime_df = pd.read_csv(realtime_file)
                logger.info(f'ðŸ“¡ Loaded real-time data: {len(realtime_df)} records')
                
                # Merge datasets
                combined_df = pd.concat([historical_df, realtime_df], ignore_index=True)
                
                # Remove duplicates based on timestamp
                combined_df = combined_df.drop_duplicates(subset=['timestamp'], keep='last')
                
                logger.info(f'ðŸ”„ Merged datasets: {len(combined_df)} total records')
            else:
                logger.warning(f'âš ï¸ Real-time data file not found: {realtime_file}')
                logger.info('ðŸ“¦ Using only historical data for training...')
                combined_df = historical_df
            
            # Sort by timestamp and drop the most recent (partial) hour to avoid incomplete data
            combined_df['timestamp'] = pd.to_datetime(combined_df['timestamp'])
            combined_df = combined_df.sort_values('timestamp')
            if len(combined_df) > 0:
                latest_ts = combined_df['timestamp'].max()
                mask_not_latest = combined_df['timestamp'] < latest_ts
                dropped = len(combined_df) - mask_not_latest.sum()
                combined_df = combined_df.loc[mask_not_latest]
                logger.info(f'ðŸ§¹ Dropped {dropped} record(s) from the most recent hour to guard against partial data')
            
            # Save combined dataset
            combined_df.to_csv('data_repositories/processed/complete_training_dataset.csv', index=False)
            
            logger.info(f'âœ… Final dataset: {len(combined_df)} records')
            logger.info(f'ðŸ“… Date range: {combined_df[\"timestamp\"].min()} to {combined_df[\"timestamp\"].max()}')
            
        except Exception as e:
            logger.error(f'âŒ Data merging failed: {e}')
            sys.exit(1)
        "
        
    - name: Feature engineering on complete dataset
      run: |
        python -c "
        import sys
        sys.path.append('.')
        import os
        import pandas as pd
        import shutil
        import logging
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        logger.info('ðŸ”§ Starting feature engineering on complete dataset...')
        
        try:
            # Check if complete training dataset exists
            training_file = 'data_repositories/processed/complete_training_dataset.csv'
            
            if not os.path.exists(training_file):
                logger.error(f'âŒ Complete training dataset not found: {training_file}')
                sys.exit(1)
            
            # Copy complete training dataset to merged_data.csv for feature engineering
            merged_file = 'data_repositories/processed/merged_data.csv'
            shutil.copy(training_file, merged_file)
            logger.info(f'ðŸ“‹ Copied complete training dataset ({training_file}) to {merged_file} for feature engineering')
            
            # Initialize feature engineer
            from phase2_enhanced_feature_engineering import EnhancedFeatureEngineer
            feature_engineer = EnhancedFeatureEngineer()
            
            # Run feature engineering pipeline
            success = feature_engineer.run_pipeline()
            
            if success:
                logger.info('âœ… Feature engineering completed successfully')
            else:
                logger.error('âŒ Feature engineering failed')
                sys.exit(1)
                
        except Exception as e:
            logger.error(f'âŒ Feature engineering error: {e}')
            sys.exit(1)
        "
        
    - name: Run Phase 4 training pipeline (multi-model + tuning + ensemble)
      run: |
        python phase4_cicd_pipeline_integration.py

    - name: Promote best model to latest
      run: |
        python -c "
        import os, sys, glob, shutil, logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        candidates = glob.glob('deployment/*/production_model.pkl')
        if not candidates:
            logger.error('âŒ No production_model.pkl found in deployment/*/. Phase 4 pipeline may have failed.')
            sys.exit(1)
        latest = sorted(candidates)[-1]
        os.makedirs('deployment', exist_ok=True)
        shutil.copy(latest, 'deployment/latest_model.pkl')
        logger.info(f'âœ… Promoted {latest} to deployment/latest_model.pkl')
        "
        
    - name: Validate model performance (from Phase 4 report)
      run: |
        python -c "
        import glob, json, logging, sys
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        reports = sorted(glob.glob('cicd_pipeline_report_*.json'))
        if not reports:
            logger.error('âŒ No Phase 4 pipeline reports found (cicd_pipeline_report_*.json)')
            sys.exit(1)
        latest = reports[-1]
        with open(latest, 'r') as f:
            data = json.load(f)
        best = float(data.get('performance_summary', {}).get('best_performance', 0.0))
        logger.info(f'ðŸ“Š Phase 4 best test RÂ²: {best:.4f}')
        if best < 0.85:
            logger.warning('âš ï¸ Best performance below threshold (0.85).')
        else:
            logger.info('âœ… Performance threshold met (â‰¥0.85).')
        "
        
    - name: Commit model and reports
      run: |
        git config --local user.email "github-actions[bot]@users.noreply.github.com"
        git config --local user.name "github-actions[bot]"
        
        # Add model files and reports (force add to override .gitignore)
        git add -f data_repositories/models/ || true
        git add -f deployment/ || true
        git add -f cicd_pipeline_report_*.json || true
        
        # Commit with timestamp
        git commit -m "ðŸ¤– Model retraining: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" || echo "No changes to commit"
        
        # Pull latest changes before pushing to avoid conflicts
        git pull origin main --rebase || git pull origin main --allow-unrelated-histories
        
        # Push to repository using GITHUB_TOKEN
        git remote set-url origin https://x-access-token:${GITHUB_TOKEN}@github.com/${{ github.repository }}
        git push origin main
        
    - name: Create training report
      run: |
        echo "# Model Training Report" > model_training_report.md
        echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> model_training_report.md
        echo "**Status:** âœ… Completed" >> model_training_report.md
        echo "" >> model_training_report.md
        echo "## Training Summary" >> model_training_report.md
        echo "- **Pipeline:** Phase 4 CI/CD (RF baseline, XGBoost + LightGBM tuning, ensembles)" >> model_training_report.md
        echo "- **Training Strategy:** Incremental time-series training on merged historical + latest data" >> model_training_report.md
        echo "- **Data Source:** Historical + Real-time merged" >> model_training_report.md
        echo "- **Outputs:** deployment/<timestamp>/production_model.pkl and deployment/latest_model.pkl" >> model_training_report.md
        echo "- **Performance Gate:** Best test RÂ² â‰¥ 0.85 (soft gate)" >> model_training_report.md
        
    - name: Upload training report
      uses: actions/upload-artifact@v4
      with:
        name: model-training-report
        path: model_training_report.md
