name: Data Collection Pipeline

on:
  schedule:
    - cron: '0 * * * *'  # Run every hour
  workflow_dispatch:  # Allow manual trigger

# Prevent multiple runs from overlapping
concurrency:
  group: ${{ github.workflow }}-${{ github.ref }}
  cancel-in-progress: true

jobs:
  collect_data:
    runs-on: ubuntu-latest
    timeout-minutes: 10  # Timeout after 10 minutes
    permissions:
      contents: read
      issues: write
    
    steps:
    - uses: actions/checkout@v4
    
    - name: Set up Python
      uses: actions/setup-python@v5
      with:
        python-version: '3.10'
        cache: 'pip'
    
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
    
    - name: Create data repository directories
      run: |
        mkdir -p data_repositories/hourly_data/{raw,processed,metadata}
        mkdir -p data_repositories/merged_data/{raw,processed,metadata}
    
    - name: Run data collection and merge
      env:
        OPENWEATHER_API_KEY: ${{ secrets.OPENWEATHER_API_KEY }}
      run: |
        # Run hourly collection
        python data_collection.py
        
        # Merge with existing data
        python merge_data.py
    
    - name: Check for data files
      run: |
        if [ ! -d "data_repositories" ]; then
          echo "Data repositories directory not found!"
          exit 1
        fi
        echo "Contents of data repositories:"
        ls -R data_repositories/
    
    - name: Upload data artifacts
      uses: actions/upload-artifact@v4
      if: always()
      with:
        name: aqi-data-${{ github.run_number }}-${{ github.run_attempt }}
        path: |
          data_repositories/hourly_data/
          data_repositories/merged_data/
        retention-days: 7
    
    - name: Check data quality
      if: success()
      run: |
        python - <<EOF
        import pandas as pd
        import sys
        import json
        from data_validation import DataValidator

        try:
            # Load the merged dataset
            df = pd.read_csv('data_repositories/merged_data/processed/merged_data.csv')
            df['timestamp'] = pd.to_datetime(df['timestamp'])
            
            # Load metadata
            with open('data_repositories/merged_data/metadata/dataset_info.json', 'r') as f:
                metadata = json.load(f)
            
            print("\nðŸ“Š Dataset Overview:")
            print(f"Records: {len(df):,}")
            print(f"Date Range: {df['timestamp'].min()} to {df['timestamp'].max()}")
            print(f"Features: {', '.join(df.columns)}")
            
            # Validate data
            validator = DataValidator()
            is_valid, report = validator.validate_merged_data(df)
            
            print("\nðŸ” Validation Results:")
            print(f"Status: {'âœ… Passed' if is_valid else 'âŒ Failed'}")
            print("\nDetailed Report:")
            print(report)
            
            if not is_valid:
                raise ValueError("Data validation failed!")
                
        except Exception as e:
            print(f"\nâŒ Error during validation: {str(e)}")
            sys.exit(1)
        EOF
    
    - name: Notify on failure
      if: failure()
      uses: actions/github-script@v6
      with:
        script: |
          const issue = await github.rest.issues.create({
            owner: context.repo.owner,
            repo: context.repo.repo,
            title: 'Data Collection Pipeline Failed',
            body: `Pipeline failed on ${new Date().toISOString()}\n\nCheck the [workflow run](${context.serverUrl}/${context.repo.owner}/${context.repo.repo}/actions/runs/${context.runId}) for details.`
          });