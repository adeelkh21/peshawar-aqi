name: AQI Data Collection Pipeline

on:
  schedule:
    # Run every hour
    - cron: '0 * * * *'
  workflow_dispatch: # Allow manual trigger
  push:
    branches: [ main ]
    paths:
      - '.github/workflows/aqi_data_pipeline.yml'

jobs:
  collect-data:
    runs-on: ubuntu-latest
    
    steps:
    - name: Checkout repository
      uses: actions/checkout@v4
      with:
        lfs: true
        
    - name: Set up Python
      uses: actions/setup-python@v4
      with:
        python-version: '3.9'
        
    - name: Install dependencies
      run: |
        python -m pip install --upgrade pip
        pip install -r requirements.txt
        
    - name: Create data directories
      run: |
        mkdir -p data_repositories/raw
        mkdir -p data_repositories/processed
        mkdir -p data_repositories/real_time_data
        mkdir -p data_repositories/historical_data
        mkdir -p data_repositories/logs
        
    - name: Collect real-time data
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from phase1_enhanced_data_collection import EnhancedDataCollector
        from datetime import datetime
        import logging
        import os
        
        # Configure logging
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        try:
            # Initialize collector
            logger.info('🔄 Initializing data collector...')
            collector = EnhancedDataCollector()
            
            # Collect current data
            logger.info('📡 Starting hourly data collection...')
            success = collector.run_pipeline()
            
            if success:
                logger.info('✅ Hourly data collection completed successfully')
                
                # Verify data was created
                data_file = 'data_repositories/processed/merged_data.csv'
                if os.path.exists(data_file):
                    import pandas as pd
                    df = pd.read_csv(data_file)
                    logger.info(f'📊 Data file created: {len(df)} records')
                else:
                    logger.error('❌ Data file not found after collection')
                    sys.exit(1)
            else:
                logger.error('❌ Hourly data collection failed')
                sys.exit(1)
                
        except Exception as e:
            logger.error(f'❌ Data collection error: {e}')
            sys.exit(1)
        "
        
    - name: Validate collected data
      run: |
        python -c "
        import sys
        sys.path.append('.')
        from data_validation import DataValidator
        import pandas as pd
        from datetime import datetime
        import logging
        import os
        
        logging.basicConfig(level=logging.INFO)
        logger = logging.getLogger(__name__)
        
        # Validate collected data
        validator = DataValidator()
        
        # Check if new data was collected
        try:
            # Check if file exists
            data_file = 'data_repositories/processed/merged_data.csv'
            if not os.path.exists(data_file):
                logger.error(f'❌ Data file not found: {data_file}')
                sys.exit(1)
            
            df = pd.read_csv(data_file)
            logger.info(f'📊 Loaded data: {len(df)} records')
            
            validation_result = validator.validate_merged_data(df)
            logger.info(f'🔍 Validation result: {validation_result}')
            
            if validation_result.get('valid', False):
                logger.info('✅ Data validation passed')
            else:
                logger.warning(f'⚠️ Data validation issues: {validation_result}')
                # Don't exit on validation warnings, only on errors
                
        except Exception as e:
            logger.error(f'❌ Data validation failed: {e}')
            sys.exit(1)
        "
        
    - name: Commit and push data
      run: |
        git config --local user.email "action@github.com"
        git config --local user.name "GitHub Action"
        
        # Add all data files
        git add data_repositories/
        
        # Commit with timestamp
        git commit -m "🔄 Hourly data collection: $(date -u '+%Y-%m-%d %H:%M:%S UTC')" || echo "No changes to commit"
        
        # Push to repository
        git push origin main
        
    - name: Create data collection report
      run: |
        echo "# Hourly Data Collection Report" > data_collection_report.md
        echo "**Date:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> data_collection_report.md
        echo "**Status:** ✅ Completed" >> data_collection_report.md
        echo "" >> data_collection_report.md
        echo "## Data Summary" >> data_collection_report.md
        echo "- **Collection Time:** $(date -u '+%Y-%m-%d %H:%M:%S UTC')" >> data_collection_report.md
        echo "- **Data Source:** OpenWeatherMap API" >> data_collection_report.md
        echo "- **Location:** Peshawar (34.0083, 71.5189)" >> data_collection_report.md
        echo "- **Validation:** ✅ PASS" >> data_collection_report.md
        
    - name: Upload data collection report
      uses: actions/upload-artifact@v4
      with:
        name: data-collection-report
        path: data_collection_report.md
